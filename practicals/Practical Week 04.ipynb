{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Name Gender Classification\n",
    "\n",
    "We have already seen the following code for partitioning the data of name gender classification and feature extraction. The code is changed slightly so that the labels are numerical (0 for male, 1 for female). This is the format required for Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package names to /home/diego/nltk_data...\n[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('names')\n",
    "from nltk.corpus import names\n",
    "m = names.words('male.txt')\n",
    "f = names.words('female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234) # Set the random seed to allow replicability\n",
    "names = ([(name,0) for name in m] +\n",
    "         [(name,1) for name in f])\n",
    "random.shuffle(names)\n",
    "train_names = names[1000:]\n",
    "devtest_names = names[500:1000]\n",
    "test_names = names[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_character(c):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    result = [0]*(len(alphabet)+1)\n",
    "    i = alphabet.find(c.lower())\n",
    "    if i >= 0:\n",
    "        result[i] = 1\n",
    "    else:\n",
    "        result[len(alphabet)] = 1 # character is out of the alphabet\n",
    "    return result\n",
    "\n",
    "def gender_features_n(word, n=2):\n",
    "    \"Return the one-hot encodings of the last n characters\"\n",
    "    features = []\n",
    "    for i in range(n):\n",
    "        if i < len(word):\n",
    "            features = one_hot_character(word[-i-1]) + features\n",
    "        else:\n",
    "            features = one_hot_character(' ') + features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "gender_features_n(\"Mary\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Let's determine the number of features so that we can use this information when we design the neural network\n",
    "len(gender_features_n(\"Mary\", n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Simple Neural Network\n",
    "Design a simple neural network that has 55 input cells (that's the number of gender features for $n=2$, as we have seen above), and one output cell (without a hidden layer). The output cell will be used to classify the name between male (output=0) and female (output=1). This is therefore an instance of **binary classification**. Pay attention to the right activation function! This simple model, without hidden layers, is equivalent to a **logistic regression** classifier. The model summary should look like this:\n",
    "\n",
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_2 (Dense)              (None, 1)                 55        \n",
    "=================================================================\n",
    "Total params: 55\n",
    "Trainable params: 55\n",
    "Non-trainable params: 0\n",
    "```\n",
    "\n",
    "\n",
    "Compile the model and provide the right loss function. Use `'rmsprop'` as the optimiser, and include `'accuracy'` as an evaluation metric. \n",
    "Run the network **for 100 epochs** using batch size of 100, and observe the results. \n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. What is the best result on the validation set?\n",
    "2. At the epoch with best result on the validation set, what is the result on the training set?\n",
    "3. Is the system overfitting? Justify your answer.\n",
    "4. Do we really need 100 epochs? Do we need more than 100 epochs? would the system run better with less epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Write your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 1)                 55        \n=================================================================\nTotal params: 55\nTrainable params: 55\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Write additional code for the partition of the data, your experiments, and your analysis. Write the answers to the questions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: A Deeper Network\n",
    "Experiment with a network that has one hidden dense layer with a `'relu'` activation. The resulting system is no longer a logistic regression classifier, it's something more complex. Try with the following sizes in the hidden layer:\n",
    "\n",
    "* 5, 10, 20\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. Which system performed best on the dev-test set?\n",
    "2. Would you add more or less cells in the hidden layer? Justify your answer.\n",
    "3. Is this system better than the simpler system of the previous exercise? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Deep Learning with the Movie Review Corpus\n",
    "The notebook [W04L1-2-MovieReviews.ipynb](../lectures/W04L1-2-MovieReviews.ipynb) has several questions at the end, repeated below. Try to answer these, and indeed try other variants!\n",
    "\n",
    "* We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.\n",
    "* Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
    "* Try to use the `mse` loss function instead of `binary_crossentropy`.\n",
    "* Try to use the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}